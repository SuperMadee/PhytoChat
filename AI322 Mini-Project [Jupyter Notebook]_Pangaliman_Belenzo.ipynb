{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PhytoChat: A Multi-Turn RL-Based LLM for Diagnosis and Treatment of Plant Diseases\n",
    "\n",
    "### AI 322 Mini Project\n",
    "\n",
    "Ma. Madecheen S. Pangaliman \\\n",
    "Jessan Rendell G. Belenzo\n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries\n",
    "\n",
    "Clone the GitHub repository at https://github.com/SuperMadee/PhytoChat, and place this Jupyter Notebook in the `PhytoChat` directory. Model checkpoints may be downloaded from https://drive.google.com/drive/folders/1mcexnpnd-XcokrALc6BY68191b-Tri4y?usp=drive_link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CUDA Device(s)\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "os.environ['HF_TOKEN'] = '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'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawling Raw Data\n",
    "\n",
    "Crawl data from the internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:33<00:00,  1.78s/it]\n"
     ]
    }
   ],
   "source": [
    "import trafilatura\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "with open('data/crawled/url_list.txt', 'r') as f:\n",
    "    urls = f.read().split('\\n')\n",
    "\n",
    "data = []\n",
    "for url in tqdm(urls):\n",
    "    try:\n",
    "        downloaded = trafilatura.fetch_url(url)\n",
    "        text = trafilatura.extract(downloaded)\n",
    "        data.append({\n",
    "            'title': url,\n",
    "            'url': url,\n",
    "            'html': text\n",
    "        })\n",
    "    except:\n",
    "        print(f'Failed to download {url}')\n",
    "\n",
    "with open('data/crawled/webpages.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and parse content from the PDF files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jessan/anaconda3/envs/phytochat/lib/python3.11/site-packages/pypdfium2/_helpers/textpage.py:80: UserWarning: get_text_range() call with default params will be implicitly redirected to get_text_bounded()\n",
      "  warnings.warn(\"get_text_range() call with default params will be implicitly redirected to get_text_bounded()\")\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pypdfium2 as pdfium\n",
    "import json\n",
    "import os\n",
    "\n",
    "pdfs_path = 'data/pdfs'\n",
    "paths = glob.glob(f'{pdfs_path}/*.pdf')\n",
    "\n",
    "\n",
    "for path in paths:\n",
    "    filename = os.path.basename(path)\n",
    "    name = filename.replace('.pdf', '')\n",
    "    json_filename = filename.replace('.pdf', '.json')\n",
    "\n",
    "    data = []\n",
    "    with open(f'data/crawled/{json_filename}', 'w') as f:\n",
    "        pdf = pdfium.PdfDocument(path)\n",
    "        n_pages = len(pdf)  # get the number of pages in the document\n",
    "        for i, page in enumerate(pdf):\n",
    "            # Load a text page helper\n",
    "            textpage = page.get_textpage()\n",
    "            # Extract text from the whole page\n",
    "            text_all = textpage.get_text_range()\n",
    "            data.append({\n",
    "                'title': f\"{name} - {i:04d}\",\n",
    "                'url': filename,\n",
    "                'html': text_all\n",
    "            })\n",
    "\n",
    "    with open(f'data/crawled/{json_filename}', 'w') as f:\n",
    "        json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Generation\n",
    "\n",
    "SFT Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/jessan/anaconda3/envs/phytochat/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "INFO 06-11 22:07:29 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct)\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO 06-11 22:07:30 utils.py:660] Found nccl from library /home/jessan/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 06-11 22:07:32 selector.py:27] Using FlashAttention-2 backend.\n",
      "INFO 06-11 22:07:36 weight_utils.py:199] Using model weights format ['*.safetensors']\n",
      "INFO 06-11 22:07:40 model_runner.py:175] Loading model weights took 14.9595 GB\n",
      "INFO 06-11 22:07:44 gpu_executor.py:114] # GPU blocks: 9478, # CPU blocks: 2048\n",
      "INFO 06-11 22:07:47 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-11 22:07:47 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-11 22:08:03 model_runner.py:1017] Graph capturing finished in 16 secs.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Processed prompts: 100%|██████████████████████████| 5/5 [00:20<00:00,  4.15s/it]\n",
      "Training set size: 100\n",
      "Validation set size: 6\n",
      "Test set size: 6\n"
     ]
    }
   ],
   "source": [
    "!python generate_sft_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DPO and ArCHer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python generate_dpo_data.py\n",
    "!python generate_conversations.py\n",
    "!python generate_dpo_data_multi_turn.py\n",
    "!python combine_split_dpo_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python finetune_sft_llama.py\n",
    "!python finetune_sft_mistral.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python finetune_dpo_llama.py\n",
    "!python finetune_dpo_mistral.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ArCHer/archer\n",
    "!python scripts/run.py --config-name archer_phytochat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python generate_predictions_sft.py\n",
    "!python generate_predictions_dpo.py\n",
    "!python generate_predictions_archer_sft.py\n",
    "!python generate_predictions_archer_dpo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation using METEOR and BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jessan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jessan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jessan/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "---\n",
      "BLEU and METEOR Scores on SFT test data:\n",
      "data/predictions/vanilla_mistral_predictions_sft.json\n",
      "BLEU: 0.017343051048786837\n",
      "METEOR: 0.22207974599469948\n",
      "\n",
      "data/predictions/sft_mistral_predictions_sft.json\n",
      "BLEU: 0.05736011425608496\n",
      "METEOR: 0.231967200266474\n",
      "\n",
      "data/predictions/dpo_mistral_predictions_sft.json\n",
      "BLEU: 0.019063562070634525\n",
      "METEOR: 0.2246878429356538\n",
      "\n",
      "data/predictions/archer_mistral_predictions_sft.json\n",
      "BLEU: 0.018706533694269167\n",
      "METEOR: 0.2217517769103468\n",
      "\n",
      "data/predictions/vanilla_llama_predictions_sft.json\n",
      "BLEU: 0.032966435021055805\n",
      "METEOR: 0.18702997127730894\n",
      "\n",
      "data/predictions/sft_llama_predictions_sft.json\n",
      "BLEU: 0.012579519074746207\n",
      "METEOR: 0.19228931412365838\n",
      "\n",
      "data/predictions/dpo_llama_predictions_sft.json\n",
      "BLEU: 0.0329763693485511\n",
      "METEOR: 0.1865908657025716\n",
      "\n",
      "data/predictions/archer_llama_predictions_sft.json\n",
      "BLEU: 0.021965214086794752\n",
      "METEOR: 0.18211155686932914\n",
      "\n",
      "---\n",
      "BLEU and METEOR Scores on DPO test data:\n",
      "data/predictions/vanilla_mistral_predictions_dpo.json\n",
      "BLEU: 0.01887392160027285\n",
      "METEOR: 0.206193242706717\n",
      "\n",
      "data/predictions/sft_mistral_predictions_dpo.json\n",
      "BLEU: 0.015777902090326538\n",
      "METEOR: 0.15801865066300608\n",
      "\n",
      "data/predictions/dpo_mistral_predictions_dpo.json\n",
      "BLEU: 0.020093997888391675\n",
      "METEOR: 0.2045763383463058\n",
      "\n",
      "data/predictions/archer_mistral_predictions_dpo.json\n",
      "BLEU: 0.015409329993870012\n",
      "METEOR: 0.2128366034277262\n",
      "\n",
      "data/predictions/vanilla_llama_predictions_dpo.json\n",
      "BLEU: 0.01478771759691825\n",
      "METEOR: 0.12037364182318122\n",
      "\n",
      "data/predictions/sft_llama_predictions_dpo.json\n",
      "BLEU: 0.04244675889117481\n",
      "METEOR: 0.21558298362502584\n",
      "\n",
      "data/predictions/dpo_llama_predictions_dpo.json\n",
      "BLEU: 0.059833968415818305\n",
      "METEOR: 0.17785573169322516\n",
      "\n",
      "data/predictions/archer_llama_predictions_dpo.json\n",
      "BLEU: 0.046219516768041195\n",
      "METEOR: 0.22564602706814552\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python evaluate_models.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phytochat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
