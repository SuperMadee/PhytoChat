{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4172a70835604b6796f4917746627bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "cache_dir = '/raid/ovod/playground/data/.cache/huggingface/hub'\n",
    "model_id = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "token = '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'\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    cache_dir=cache_dir,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation='flash_attention_2',\n",
    "    token=token\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=token)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \"Thank you, PhytoChat. I will remove the affected leaves and apply a fungicide as you suggested. I will also be more careful with watering to prevent overwatering. I appreciate your help.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "input = \"\"\"<s>[INST]You are User, a plant grower who is concerned with the health of your plant. Respond to PhytoChat by providing more information about your plant and say thank you once satisfied.\n",
    "\n",
    "An example conversation is as follows:\n",
    "PhytoChat: Hello! How can I help you today?\n",
    "User: My plant has yellow spots on its leaves. What should I do?\n",
    "PhytoChat: May I ask if the spots are on the upper or lower side of the leaves?\n",
    "User: They are on the upper side.\n",
    "PhytoChat: The yellow spots on the upper side of the leaves may indicate a fungal infection. You can try removing the affected leaves and applying a fungicide.\n",
    "User: Thank you, PhytoChat!\n",
    "\n",
    "Please continue this conversation by responding to PhytoChat as User.\n",
    "PhytoChat: Hello, how can I help you today?\n",
    "User: My plant has yellow spots on its leaves. Can you help me?\n",
    "PhytoChat: I see. The yellow spots on the upper leaves may be a sign of a fungal infection. You can try removing the affected leaves and applying a fungicide to prevent the infection from spreading. Make sure to keep the plant watered properly and avoid overwatering, as too much moisture can exacerbate the issue.\n",
    "\n",
    "Please answer in the following format:\n",
    "{\n",
    "\"Response\": \"Your Response\",\n",
    "}[/INST]\"\"\"\n",
    "encoder_ids = tokenizer([input], padding=True, return_tensors='pt').to(model.device)\n",
    "outputs = tokenizer.batch_decode(model.generate(input_ids=encoder_ids['input_ids'], attention_mask=encoder_ids['attention_mask'],\\\n",
    "                                                max_new_tokens=64, do_sample = False), skip_special_tokens= True)\n",
    "print(outputs[0].split('\"Response\":')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [f\"Your hometown is Manila. Answer the following question in one sentence. Do not say the name of your hometown in your answer. What is the predominant language spoken in the city where you are from?\"]\n",
    "encoder_ids = tokenizer(inputs, padding=True, return_tensors='pt').to(model.device)\n",
    "outputs = tokenizer.batch_decode(model.generate(input_ids=encoder_ids['input_ids'], attention_mask=encoder_ids['attention_mask'],\\\n",
    "                                                max_new_tokens=64, do_sample = False), skip_special_tokens= True)\n",
    "outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5'\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "cache_dir = '/raid/ovod/playground/data/.cache/huggingface/hub'\n",
    "model_id = 'google/flan-t5-small'\n",
    "token = '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'\n",
    "oracle_path = '/raid/ovod/playground/data/jessan/ArCHer/dataset/ArCHer_public/city_t5_oracle.pt'\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_id,\n",
    "    cache_dir=cache_dir,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model.load_state_dict(torch.load(oracle_path)['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "import random\n",
    "from typing import Optional, Dict\n",
    "import time\n",
    "from openai import OpenAI\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import concurrent.futures\n",
    "\n",
    "MISTRAL_TWENTY_QUESTIONS_TEMPLATE = \"\"\"<s>[INST]You are PhytoChat, a botanist who is an expert in plant disease management. Respond to the user's questions about plant diseases and provide them with the correct information.\n",
    "\n",
    "An example conversation is as follows:\n",
    "PhytoChat: Hello! How can I help you today?\n",
    "User: My plant has yellow spots on its leaves. What should I do?\n",
    "PhytoChat: May I ask if the spots are on the upper or lower side of the leaves?\n",
    "User: They are on the upper side.\n",
    "PhytoChat: The yellow spots on the upper side of the leaves may indicate a fungal infection. You can try removing the affected leaves and applying a fungicide.\n",
    "User: Thank you, PhytoChat!\n",
    "\n",
    "Please continue this conversation by responding to the user. \n",
    "{obs}\n",
    "Please answer in the following format:\n",
    "{\n",
    "\"Response\": \"Your Response\",\n",
    "}[/INST]\n",
    "\"\"\"\n",
    "\n",
    "def mistral_twenty_questions_decode_actions(output):\n",
    "    \"\"\"\n",
    "    Decode the actions from the output of the model.\n",
    "    \"\"\"\n",
    "    actions = []\n",
    "    for a in output:\n",
    "        action = a.split('\"Response\":')[-1]\n",
    "        action = action.split(\"}\")[0].strip()\n",
    "        action = action.strip().replace('\"', '')\n",
    "        actions.append(action)\n",
    "    return actions\n",
    "\n",
    "# openai.util.logger.setLevel(logging.WARNING)\n",
    "CITY_LIST = ['Seoul, South Korea',\n",
    " 'Sao Paulo, Brazil',\n",
    " 'Bombay, India',\n",
    " 'Jakarta, Indonesia',\n",
    " 'Karachi, Pakistan',\n",
    " 'Moscow, Russia',\n",
    " 'Istanbul, Turkey',\n",
    " 'Shanghai, China',\n",
    " 'Tokyo, Japan',\n",
    " 'Bangkok, Thailand',\n",
    " 'Beijing, China',\n",
    " 'Delhi, India',\n",
    " 'London, UK',\n",
    " 'Cairo, Egypt',\n",
    " 'Tehran, Iran',\n",
    " 'Bogota, Colombia',\n",
    " 'Bandung, Indonesia',\n",
    " 'Tianjin, China',\n",
    " 'Lima, Peru',\n",
    " 'Lahore, Pakistan',\n",
    " 'Bogor, Indonesia',\n",
    " 'Santiago, Chile',\n",
    " 'Shenyang, China',\n",
    " 'Calcutta, India',\n",
    " 'Wuhan, China',\n",
    " 'Sydney, Australia',\n",
    " 'Guangzhou, China',\n",
    " 'Singapore, Singapore',\n",
    " 'Madras, India',\n",
    " 'Baghdad, Iraq',\n",
    " 'Pusan, South Korea',\n",
    " 'Yokohama, Japan',\n",
    " 'Dhaka, Bangladesh',\n",
    " 'Berlin, Germany',\n",
    " 'Alexandria, Egypt',\n",
    " 'Bangalore, India',\n",
    " 'Malang, Indonesia',\n",
    " 'Hyderabad, India',\n",
    " 'Chongqing, China',\n",
    " 'Haerbin, China',\n",
    " 'Ankara, Turkey',\n",
    " 'Buenos Aires, Argentina',\n",
    " 'Chengdu, China',\n",
    " 'Ahmedabad, India',\n",
    " 'Casablanca, Morocco',\n",
    " 'Chicago, USA',\n",
    " 'Xian, China',\n",
    " 'Madrid, Spain',\n",
    " 'Surabaya, Indonesia',\n",
    " 'Pyong Yang, North Korea',\n",
    " 'Nanjing, China',\n",
    " 'Kinshaha, Congo',\n",
    " 'Rome, Italy',\n",
    " 'Taipei, China',\n",
    " 'Osaka, Japan',\n",
    " 'Kiev, Ukraine',\n",
    " 'Yangon, Myanmar',\n",
    " 'Toronto, Canada',\n",
    " 'Zibo, China',\n",
    " 'Dalian, China',\n",
    " 'Taega, South Korea',\n",
    " 'Addis Ababa, Ethopia',\n",
    " 'Jinan, China',\n",
    " 'Salvador, Brazil',\n",
    " 'Inchon, South Korea',\n",
    " 'Semarang, Indonesia',\n",
    " 'Giza, Egypt',\n",
    " 'Changchun, China',\n",
    " 'Havanna, Cuba',\n",
    " 'Nagoya, Japan',\n",
    " 'Belo Horizonte, Brazil',\n",
    " 'Paris, France',\n",
    " 'Tashkent, Uzbekistan',\n",
    " 'Fortaleza, Brazil',\n",
    " 'Sukabumi, Indonesia',\n",
    " 'Cali, Colombia',\n",
    " 'Guayaquil, Ecuador',\n",
    " 'Qingdao, China',\n",
    " 'Izmir, Turkey',\n",
    " 'Cirebon, Indonesia',\n",
    " 'Taiyuan, China',\n",
    " 'Brasilia, Brazil',\n",
    " 'Bucuresti, Romania',\n",
    " 'Faisalabad, Pakistan',\n",
    " 'Medan, Indonesia',\n",
    " 'Houston, USA',\n",
    " 'Mashhad, Iran',\n",
    " 'Medellin, Colombia',\n",
    " 'Kanpur, India',\n",
    " 'Budapest, Hungary',\n",
    " 'Caracas, Venezuela']\n",
    "\n",
    "concerns = [\n",
    "    \"My plant has yellow spots on its leaves. Can you help me?\",\n",
    "    \"I think my plant has a fungal infection.\",\n",
    "    \"My plant has black spots on its leaves. What should I do?\",\n",
    "]\n",
    "\n",
    "INITIAL_STR = f\"\"\"Questions:\n",
    "User:{random.choice(concerns)}\n",
    "\"\"\"\n",
    "\n",
    "class GuessMyCityEnv():\n",
    "    def __init__(\n",
    "        self, \n",
    "        # word_list,  \n",
    "        max_conversation_length: int=20,\n",
    "    ):\n",
    "        self.city_list = CITY_LIST\n",
    "        self.max_conversation_length = max_conversation_length\n",
    "        self.random = random.Random(None)\n",
    "        self.count = 0\n",
    "        self.curr_word = None\n",
    "        self.history = ''\n",
    "        self.done = True\n",
    "\n",
    "    def is_correct(self, question):\n",
    "        #check for the last word\n",
    "        # cut out punctuations at the end\n",
    "        while len(question) > 0 and not question[-1].isalpha():\n",
    "            question = question[:-1]\n",
    "\n",
    "        if len(question) == 0:\n",
    "            return False\n",
    "        # this is the name of the city\n",
    "        word = self.curr_word.lower().split(\",\")[0]\n",
    "        return word in question.lower()\n",
    "        # guess = question.split(\" \")[-1].lower()\n",
    "        # return guess in self.curr_word.lower().split(\",\")[0] and len(guess) >= 3\n",
    "\n",
    "    def _step(self, question, answer):\n",
    "        answer = answer.split('?')[-1].strip() # Remove prompt from output\n",
    "        if self.done:\n",
    "            return None\n",
    "        # if self.curr_word.lower().split(\",\")[0] in answer.lower():\n",
    "        #     answer = \"I can't answer that question.\"\n",
    "        self.count+=1\n",
    "        # self.history += question + ' ' + answer + '\\n'\n",
    "        self.history += f\"PhytoChat: {question}\\nUser: {answer}\\n\"\n",
    "        done = self.is_correct(question)\n",
    "        reward = -1\n",
    "        #if correct reward is -1\n",
    "        if done:\n",
    "            reward = 0\n",
    "        self.done = done or self.count == self.max_conversation_length\n",
    "        return  self.history, reward, self.done\n",
    "        \n",
    "    def reset(self, idx : Optional[int]=None):\n",
    "        self.count = 0 \n",
    "        if idx is not None:\n",
    "            self.curr_word = self.city_list[idx]\n",
    "        else:\n",
    "            self.curr_word = self.random.choice(self.city_list)\n",
    "        self.history = INITIAL_STR \n",
    "        self.done = False\n",
    "        return INITIAL_STR\n",
    "        # return (Text(INITIAL_STR, is_action=False),)\n",
    "\n",
    "\n",
    "class BatchedGuessMyCityEnv():\n",
    "    def __init__(\n",
    "        self, \n",
    "        env_load_path: str,\n",
    "        device,\n",
    "        cache_dir: str,\n",
    "        max_conversation_length: int=5,\n",
    "        bsize: int=4,\n",
    "    ):\n",
    "        self.env_list = [GuessMyCityEnv(max_conversation_length) for _ in range(bsize)]\n",
    "        self.bsize = bsize\n",
    "\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            bnb_4bit_quant_type='nf4',\n",
    "            bnb_4bit_use_double_quant=True\n",
    "        )\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "            quantization_config=bnb_config,\n",
    "            device_map='auto',\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            cache_dir=\"/raid/ovod/playground/data/.cache/huggingface/hub\",\n",
    "            attn_implementation='flash_attention_2'\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.padding_side = \"left\"\n",
    "\n",
    "\n",
    "    def generate_answers(self, questions):\n",
    "        histories = [env.history for env in self.env_list]\n",
    "        inputs = [f\"{history}\\nPhytoChat: {question}\\nUser: \" for history, question in zip(histories, questions)]\n",
    "        encoder_ids = self.tokenizer(inputs ,padding=True, return_tensors='pt').to(self.model.device)\n",
    "        outputs = self.tokenizer.batch_decode(self.model.generate(input_ids=encoder_ids['input_ids'], attention_mask=encoder_ids['attention_mask'],\\\n",
    "                                                                max_new_tokens=64, do_sample = False), skip_special_tokens= True)\n",
    "        return [output.split('User:')[-1].strip() for output in outputs]\n",
    "\n",
    "    def reset(self, idx: Optional[int] = None):\n",
    "        return [env.reset(idx) for env in self.env_list]\n",
    "    \n",
    "    def step(self, questions):\n",
    "        answers = self.generate_answers(questions)\n",
    "        # print(\"Step once!\")\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor: \n",
    "            jobs = [executor.submit(env._step, q, a) for env, q, a in zip(self.env_list, questions, answers)]\n",
    "            results = [job.result() for job in jobs]\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BatchedGuessMyCityEnv('', 'cuda', '/raid/ovod/playground/data/.cache/huggingface/hub')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in env.env_list:\n",
    "    e._step()\n",
    "    print(e.history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phytochat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
